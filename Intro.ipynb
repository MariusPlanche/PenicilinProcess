{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3861a081",
   "metadata": {},
   "source": [
    "# Data Analysis of insulin process production\n",
    "\n",
    "## Goal\n",
    "\n",
    "Potential projects: Raman spectrometer soft sensor development, Faulty batches detection (before too late), \n",
    "\n",
    "## About the data:\n",
    "\n",
    "The data was downloaded from [Kaggle](https://www.kaggle.com/datasets/stephengoldie/big-databiopharmaceutical-manufacturing/data). It consists of data for 100 batches of insulin, generated by [IndPenSim](http://www.industrialpenicillinsimulation.com/)\n",
    "\n",
    "![Variables and Parameters](IndPenSim_input_outputs_V2.png) \n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "Goldrick S., Stefan, A., Lovett D., Montague G., Lennox B. (2015) The development of an industrial-scale fed-batch fermentation simulation Journal of Biotechnology, 193:70-82.\n",
    "\n",
    "Goldrick S., Duran-Villalobos C., K. Jankauskas, Lovett D., Farid S. S, Lennox B., (2019) Modern day control challenges for industrial-scale fermentation processes. Computers and Chemical Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc971e",
   "metadata": {},
   "source": [
    "## 1 - Visualisation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e44a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Output\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import scipy.stats as stats\n",
    "\n",
    "from helper import plot_batch_data\n",
    "\n",
    "data = pd.read_csv('data/100_Batches_IndPenSim_V3.csv', usecols=range(39)) \n",
    "data_summary = pd.read_csv('data/100_Batches_IndPenSim_Statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e79d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrites Batch ID's correctly\n",
    "data = data.rename(columns={'2-PAT control(PAT_ref:PAT ref)': 'Batch reference', # Mistaken attribution\n",
    "                            'Batch reference(Batch_ref:Batch ref)':'Faulty batch', # Mistaken attribution + seems to indicate if a batch is faulty\n",
    "                            })\n",
    "\n",
    "# Drop superfluous/unusable columns\n",
    "data.drop([' 1-Raman spec recorded'], axis=1, inplace=True) # I don't see the point of this column, seems to be a duplicate of Batch reference\n",
    "data.drop(['Batch ID'], axis=1, inplace=True) # Mysterious column; is not actually a batch ID, closely follows penicilin concentration\n",
    "data.drop(['Fault flag'], axis=1, inplace=True) # Mysterious column; is not actually a binary flag, closely follows penicilin concentration\n",
    "\n",
    "# Changes the ID of the columns for more comprehensive names\n",
    "data = data.rename(columns={'0 - Recipe driven 1 - Operator controlled(Control_ref:Control ref)':'Operator controlled',\n",
    "                            'Fault reference(Fault_ref:Fault ref)':'Faulty measure'})\n",
    "\n",
    "# Correct data readability for a categorical (binary) variable:\n",
    "data = data.rename(columns={'1- No Raman spec': 'Raman spec recorded'})\n",
    "data['Raman spec recorded'] = data['Raman spec recorded'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data:\n",
    "missing_data = data.isnull().sum()\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f625aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep data from the campaign 4 (same as in the paper): Fixed duration, recipe driven\n",
    "data = data[data['Operator controlled'] == 0]\n",
    "data = data[data['Raman spec recorded'] == 0] # Raman spec controlled batches are also marked as \"recipe driven\"\n",
    "grouped = data.groupby('Batch reference')\n",
    "filtered_groups = [group for name, group in grouped if len(group) == 1150] # Only keep 230h long batches\n",
    "\n",
    "data = pd.concat(filtered_groups)\n",
    "del grouped, filtered_groups\n",
    "\n",
    "# Drop columns with discrete/categorical variables:\n",
    "data.drop(['Faulty measure',\n",
    "           'Operator controlled',\n",
    "           'Raman spec recorded'],\n",
    "            axis=1, inplace=True)\n",
    "\n",
    "# Drop columns with delayed measurements (offline) and values provided by the Raman spectrometer:\n",
    "data.drop(['PAA concentration offline(PAA_offline:PAA (g L^{-1}))',\n",
    "           'Offline Penicillin concentration(P_offline:P(g L^{-1}))',\n",
    "           'Offline Biomass concentratio(X_offline:X(g L^{-1}))',\n",
    "           'Viscosity(Viscosity_offline:centPoise)',\n",
    "           'NH_3 concentration off-line(NH3_offline:NH3 (g L^{-1}))',\n",
    "           'Substrate concentration(S:g/L)',\n",
    "           'Penicillin concentration(P:g/L)'], axis=1, inplace=True)\n",
    "\n",
    "# Drop columns with constant values:\n",
    "data.drop(['Agitator RPM(RPM:RPM)', 'Ammonia shots(NH3_shots:kgs)'], axis=1, inplace=True)\n",
    "\n",
    "# Creates a copy of the data so that the plot in the following cell can stay interactive\n",
    "copy_data = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch_data(copy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458f489",
   "metadata": {},
   "source": [
    "Important observation: Most batches have a duration of 230h, but some last for slightly longer/shorter times.\n",
    "\n",
    "- Agitation: Always 100RPM, not useful\n",
    "- Water for injection/dilution: Same trajectory for all batches\n",
    "- Air head pressure: Same trajectory for all batches\n",
    "- Dumped broth flow: Same trajectory for all batches\n",
    "- Ammonia shots: Same trajectory for all batches\n",
    "- PAA concentration offline does not show any data on the plot => because taken every 4 hours\n",
    "- NH3 concentration offline does not show any data on the plot => because taken every 4 hours\n",
    "- Penicilin concentration offline does not show any data on the plot => because taken every 4 hours\n",
    "- Biomass concentration offline does not show any data on the plot => because taken every 4 hours\n",
    "- Viscosity offline does not show any data on the plot => because taken every 4 hours\n",
    "- What is Batch ID? Seems to be a numerical value related to peniciline concentration\n",
    "- What is Fault flag? Seems to be a numerical value related to peniciline concentration\n",
    "\n",
    "//\n",
    "\n",
    "- Vessel weight and vessel volume should be highly correlated\n",
    "- Generated heat, oxygen uptake rate and CO2% in outlet should be highly correlated\n",
    "\n",
    "\n",
    "Some features in the data are there because the data comes from a bioprocess simulation; in reality, these features cannot be measured properly. These features are: \n",
    "- On-line Penicillin concentration (We assume we do not have a Raman spectrometer in the tank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d50add",
   "metadata": {},
   "source": [
    "## 2 - Multiway PCA\n",
    "\n",
    "We are going to apply PCA on the data, to visualize the data and see if we can detect the \"good\" and the \"bad\" batches before they end. To do this, we are going to unfold the data, so that we have a row for each batch and a column for every parameter at every time point. It means that every parameter at every timepoint are vectors orthogonal to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c448b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=['Batch reference', 'Time (h)'])\n",
    "filtered_operator_driven_data = data # We save this for the later Fourier analysis\n",
    "\n",
    "# Pivot the DataFrame\n",
    "variable_names = list(set(data.columns.to_list()) - set('Time (h)'))\n",
    "data = data.pivot_table(index='Batch reference', columns='Time (h)', values=variable_names)\n",
    "\n",
    "# Flatten the MultiIndex columns:\n",
    "data.columns = [f'{var}_t{time}' for var, time in data.columns]\n",
    "\n",
    "# We keep only one \"Faulty batch\" column:\n",
    "columns_to_drop = data.filter(regex=\"^Faulty batch\").columns\n",
    "data = data.drop(columns_to_drop[1:], axis=1)\n",
    "data = data.rename(columns={\"Faulty batch_t0.2\": \"Faulty batch\"})\n",
    "del columns_to_drop\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes columns with standard deviation of 0, not useful for prediction\n",
    "# We are able to drop around 40% of the columns!\n",
    "data = data.loc[:, data.std() != 0]\n",
    "\n",
    "# Normalization\n",
    "columns_to_normalize = data.columns.difference(['Faulty batch'])\n",
    "data[columns_to_normalize] = (data[columns_to_normalize] - data[columns_to_normalize].mean()) / data[columns_to_normalize].std()\n",
    "\n",
    "NOC_batches = data[data['Faulty batch'] == 0].drop(\"Faulty batch\", axis=1)\n",
    "faulty_batches = data[data['Faulty batch'] == 1].drop(\"Faulty batch\", axis=1)\n",
    "del data\n",
    "\n",
    "# PCA\n",
    "pca = PCA()\n",
    "pca.fit(NOC_batches) # Fit with only NOC batches\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Cumulative explained variance:\", explained_variance_ratio.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52293d4",
   "metadata": {},
   "source": [
    "We can see that around 80% of the variance can be explained by 7 components. We are going to use the first 7 PC to describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043d1d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_PC = 7\n",
    "PCA_statistics = pd.DataFrame(columns=['Batch reference', 'Q stat', 'Q threshold', 'T2 stat', 'T2 threshold', 'Faulty batch'])\n",
    "loo=LeaveOneOut()\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(loo.split(NOC_batches)):\n",
    "    # Use .iloc to index by position\n",
    "    train_data = NOC_batches.iloc[train_index]\n",
    "    test_data = NOC_batches.iloc[test_index]\n",
    "    \n",
    "    # Initialize and fit the PCA model to retreive \n",
    "    # all the eigenvalues of the covariance matrix\n",
    "    pca = PCA()\n",
    "    pca.fit(train_data)\n",
    "    #eigenvalues_corr_matrix = pca.explained_variance_ratio_\n",
    "    eigenvalues_cov_matrix = pca.explained_variance_\n",
    "    eigenvalues_retained_PC = eigenvalues_cov_matrix[:nb_PC]\n",
    "    eigenvalues_discarded_PC = eigenvalues_cov_matrix[nb_PC:]\n",
    "    \n",
    "    # re_fit the PCA (ugly, but only way to have all eigenvalues \n",
    "    # AND projecting on a subset of PC)\n",
    "    pca = PCA(n_components=nb_PC)\n",
    "    pca.fit(train_data)\n",
    "    \n",
    "    # Transform the data\n",
    "    test_data_transformed = pca.transform(test_data)\n",
    "    train_data_transformed = pca.transform(train_data)\n",
    "    faulty_batches_transformed = pca.transform(faulty_batches)\n",
    "    \n",
    "    # Reconstruct the data from the PCA model\n",
    "    test_data_reconstructed = pca.inverse_transform(test_data_transformed)\n",
    "    train_data_reconstructed = pca.inverse_transform(train_data_transformed)\n",
    "    faulty_batches_reconstructed = pca.inverse_transform(faulty_batches_transformed)\n",
    "    \n",
    "    alpha = 0.05\n",
    "    \n",
    "    # Compute the Q statistic (sum of squared residuals)\n",
    "    residuals_test_data = test_data - test_data_reconstructed\n",
    "    residuals_faulty_batches = faulty_batches - faulty_batches_reconstructed\n",
    "    Q_statistic_test_data = float(np.sum(residuals_test_data ** 2, axis=1).iloc[0])\n",
    "    \n",
    "    Q_statistics_faulty_batches = []\n",
    "    for index, f_batch_df in residuals_faulty_batches.iterrows():\n",
    "        f_batch = f_batch_df.to_numpy()\n",
    "        Q_statistic_faulty_batch = np.sum(f_batch ** 2)\n",
    "        Q_statistics_faulty_batches.append(Q_statistic_faulty_batch)\n",
    "    \n",
    "    # Compute the 95% confidence treshold for the Q statistic:\n",
    "    residuals_train_data = train_data - train_data_reconstructed\n",
    "    theta_1 = np.sum(eigenvalues_discarded_PC)\n",
    "    theta_2 = np.sum(eigenvalues_discarded_PC**2)\n",
    "    g_SPE = theta_2/theta_1\n",
    "    h_SPE = (theta_1**2)/theta_2\n",
    "    Q_threshold = g_SPE*stats.chi2.ppf(1 - alpha, h_SPE)\n",
    "    \n",
    "    # Compute the Hostelling T² statistic (Mahalanobis distance between origin and scores)\n",
    "    T2_statistic_test_data = np.sum((test_data_transformed[0] ** 2) / eigenvalues_retained_PC)\n",
    "    \n",
    "    T2_statistics_faulty_batches = []\n",
    "    for f_batch in faulty_batches_transformed:\n",
    "        T2_statistic_faulty_batch = np.sum((f_batch ** 2) / eigenvalues_retained_PC)\n",
    "        T2_statistics_faulty_batches.append(T2_statistic_faulty_batch)\n",
    "\n",
    "    # Compute the 95% confidence treshold for the T² statistic:\n",
    "    R = nb_PC\n",
    "    I = len(train_index)\n",
    "    T2_threshold = R*(I-1)/(I-R) * stats.f.ppf(1-alpha, R, I-R)\n",
    "    \n",
    "    # Store the results\n",
    "    stats_test_row = pd.DataFrame({'Batch reference':[test_data.index[0]], \n",
    "                                   'Q stat': [Q_statistic_test_data],\n",
    "                                   'Q threshold': [Q_threshold],\n",
    "                                   'T2 stat': [T2_statistic_test_data],\n",
    "                                   'T2 threshold': [T2_threshold],\n",
    "                                   'Faulty batch': [0]})\n",
    "    \n",
    "    stats_faulty_batches_rows = pd.DataFrame({'Batch reference':faulty_batches.index.to_list(), \n",
    "                                   'Q stat': Q_statistics_faulty_batches,\n",
    "                                   'Q threshold': [Q_threshold]*len(faulty_batches),\n",
    "                                   'T2 stat': T2_statistics_faulty_batches,\n",
    "                                   'T2 threshold': [T2_threshold]*len(faulty_batches),\n",
    "                                   'Faulty batch': [1]*len(faulty_batches)})\n",
    "    \n",
    "    PCA_statistics = pd.concat([PCA_statistics, stats_test_row, stats_faulty_batches_rows], ignore_index=True)\n",
    "    #PCA_statistics = pd.concat([PCA_statistics, stats_faulty_batches_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe119ed",
   "metadata": {},
   "source": [
    "We can now visualize the results. Let's see if batch outcome can be predicted from the whole time series of the process variables : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_stats = PCA_statistics.groupby(['Batch reference']).mean()\n",
    "averaged_stats = averaged_stats.reset_index(drop=True)\n",
    "\n",
    "# Plot Q statistics\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(len(averaged_stats)):\n",
    "    if averaged_stats['Faulty batch'].iloc[i] == 1.0:\n",
    "        plt.scatter(averaged_stats.index[i], \n",
    "                    averaged_stats['Q stat'].iloc[i], \n",
    "                    color='red', \n",
    "                    label='Faulty batches' if i == averaged_stats[averaged_stats['Faulty batch']==1].iloc[0].name \\\n",
    "                    else \"\")\n",
    "    else:\n",
    "        plt.scatter(averaged_stats.index[i], \n",
    "                    averaged_stats['Q stat'].iloc[i], \n",
    "                    color='blue', \n",
    "                    label='NOC batches' if i == averaged_stats[averaged_stats['Faulty batch']==0].iloc[0].name \\\n",
    "                    else \"\")\n",
    "    \n",
    "    # Plot thresholds\n",
    "    plt.hlines(y=averaged_stats['Q threshold'].iloc[i], \n",
    "               xmin=averaged_stats.index[i] - 0.5, \n",
    "               xmax=averaged_stats.index[i] + 0.5, \n",
    "               color='black', \n",
    "               linestyles='solid'),\n",
    "               #label='95% significance threshold')\n",
    "\n",
    "    \n",
    "# Calculate the average of the blue and red points\n",
    "average_blue = averaged_stats[averaged_stats['Faulty batch'] == 0.0]['Q stat'].mean()\n",
    "average_red = averaged_stats[averaged_stats['Faulty batch'] == 1.0]['Q stat'].mean()\n",
    "\n",
    "# Add a horizontal blue line at the average height of the blue points\n",
    "plt.axhline(y=average_blue, color='blue', linestyle='dotted', label='Average of Non-Faulty Batches')\n",
    "plt.axhline(y=average_red, color='red', linestyle='dotted', label='Average of faulty Batches')\n",
    "    \n",
    "# Labeling the plot\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Q Statistic')\n",
    "plt.title('Q Statistics of the PCA (whole batch length)')\n",
    "plt.xticks(averaged_stats.index)  # Set x-ticks to batch references\n",
    "plt.legend()\n",
    "\n",
    "# Hide Y axes label marks\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_tick_params(labelbottom=False)\n",
    "ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot T2 statistics:\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(len(averaged_stats)):\n",
    "    if averaged_stats['Faulty batch'].iloc[i] == 1.0:\n",
    "        plt.scatter(averaged_stats.index[i], \n",
    "                    averaged_stats['T2 stat'].iloc[i], \n",
    "                    color='red', \n",
    "                    label='Faulty batches' if i == averaged_stats[averaged_stats['Faulty batch']==1].iloc[0].name \\\n",
    "                    else \"\")\n",
    "    else:\n",
    "        plt.scatter(averaged_stats.index[i], \n",
    "                    averaged_stats['T2 stat'].iloc[i], \n",
    "                    color='blue', \n",
    "                    label='NOC batches' if i == averaged_stats[averaged_stats['Faulty batch']==0].iloc[0].name \\\n",
    "                    else \"\")\n",
    "    \n",
    "    # Plot thresholds\n",
    "    plt.hlines(y=averaged_stats['T2 threshold'].iloc[i], \n",
    "               xmin=averaged_stats.index[i] - 0.5, \n",
    "               xmax=averaged_stats.index[i] + 0.5, \n",
    "               color='black', \n",
    "               linestyles='solid')\n",
    "               #label='95% significance threshold')\n",
    "\n",
    "    \n",
    "# Calculate the average of the blue and red points\n",
    "average_blue = averaged_stats[averaged_stats['Faulty batch'] == 0.0]['T2 stat'].mean()\n",
    "average_red = averaged_stats[averaged_stats['Faulty batch'] == 1.0]['T2 stat'].mean()\n",
    "\n",
    "# Add a horizontal blue line at the average height of the blue points\n",
    "plt.axhline(y=average_blue, color='blue', linestyle='dotted', label='Average of Non-Faulty Batches')\n",
    "plt.axhline(y=average_red, color='red', linestyle='dotted', label='Average of faulty Batches')\n",
    "    \n",
    "# Labeling the plot\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('T2 Statistic')\n",
    "plt.title('T2 Statistics of the PCA (whole batch length)')\n",
    "plt.xticks(averaged_stats.index)  # Set x-ticks to batch references\n",
    "plt.legend()\n",
    "\n",
    "# Hide Y axes label marks\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_tick_params(labelbottom=False)\n",
    "ax.set_xticks([])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce83f08",
   "metadata": {},
   "source": [
    "We can observe the following two things:\n",
    "\n",
    "1) The Q statistics are almost all above the 95% confidence threshold and the T2 statistics are all below the confidence threshold regardless of the batch quality. It seems that the tests fail to capture the quality of the batches.\n",
    "\n",
    "2) The mean statistics (Q and T²) of the NOC batches are slightly lower than the faulty batches. The difference may or may not be significant, we don't have enough data to test it.\n",
    "\n",
    "In the original paper by Goldrick & al., the authors show that the T² statistic fails to capture the batch quality, as the 95% confidence threshold is never attained by any batch, faulty or not. They highlight that previous research has found the T2 statistic to underestimate faults. \\\\\n",
    "\n",
    "Their Q-statistic plot however looks different. All batches are correctly classified as faulty or NOC by the 95% confidence limit, which contrasts to our plot. Goldrick & al constructed the Q-statistic plot by showing the Q-statistic of batches used to create the PCA in the first place. This approach is misleading, as it does not check the prediction accuracy of the test; it's like formulating a hypothesis and validating it with the same data. We tested our PCA with cross-validation, in a way that reflects how newly recorded batches would compare against the PCA model. These results show that the PCA model is likely overfitting the data. This is not really surprising, as there are only 15 NOC batches and thousands of variables. So, what could we do to avoid this overfitting problem?\n",
    "\n",
    "1) Use less variables: If a fault happens in the process after 200 hours, it is more likely that the first hours of the process are not the cause of this effect. The cause-to-effect time should be limited by a certain value. This means that we can use data only from the last X measures of the process to detect faults, in a moving window manner. It would avoid adding unnecessary variables to the PCA model that most likely contain NOC data anyway.\n",
    "\n",
    "2) Normalize the data differently: Some variables have variability over time heavily influenced by noise. We can guess that based on our mechanistic understanding of the process. For example, recorded temperature should be pretty uniform, high frequency changes are noise that we do not want to have in our PCA model. To avoid that, we could apply a smoothing function to the data, or use a low-pass filter. The method of choice has to be applicable in real time, since we want real-time monitoring of the process.\n",
    "\n",
    "## 3 - Fourier Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the DFT for each batch:\n",
    "grouped_batches = filtered_operator_driven_data.groupby('Batch reference')\n",
    "fourier_all_operator_batches = []\n",
    "for name, batch in grouped_batches:\n",
    "    N = len(batch)\n",
    "    raw_fourier_batch = np.abs(np.fft.fft(batch.to_numpy(), axis=0))/N # Get the amplitude of the signal\n",
    "    fourier_operator_batch = pd.DataFrame(raw_fourier_batch, columns=filtered_operator_driven_data.columns)\n",
    "    fourier_operator_batch['Batch reference'] = batch['Batch reference'].reset_index(drop=True)\n",
    "    fourier_operator_batch['Faulty batch'] = batch['Faulty batch'].reset_index(drop=True)\n",
    "    fourier_operator_batch['Frequency (1/h)'] = np.fft.fftfreq(len(batch), d=0.2)\n",
    "    fourier_all_operator_batches.append(fourier_operator_batch)\n",
    "    \n",
    "fourier_all_operator_batches = pd.concat(fourier_all_operator_batches)\n",
    "fourier_all_operator_batches.drop(['Time (h)'], axis=1, inplace=True)\n",
    "fourier_all_operator_batches = fourier_all_operator_batches.sort_values(by=['Batch reference', 'Frequency (1/h)'])\n",
    "fourier_all_operator_batches.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Removing the DC components out of the Fourier transforms:\n",
    "columns_to_modify = fourier_all_operator_batches.columns.difference(['Frequency (1/h)', 'Batch reference', 'Faulty batch'])\n",
    "fourier_all_operator_batches.loc[fourier_all_operator_batches['Frequency (1/h)'] == 0, columns_to_modify] = 0\n",
    "fourier_all_operator_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23807d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_list_2 = fourier_all_operator_batches.columns\n",
    "variable_plot_selection_2 = widgets.Dropdown(options=variable_list_2, value = 'Temperature(T:K)')\n",
    "variable_plot_selection_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "bp = fourier_all_operator_batches.groupby('Batch reference').plot(x = 'Frequency (1/h)', y = variable_plot_selection_2.value,   ax=ax, legend = False, )\n",
    "ax.set_title('Summary of Campaigns, Frequency domain (DC removed)')\n",
    "ax.set_xlabel('Frequency (1/h)')\n",
    "ax.set_ylabel(variable_plot_selection_2.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb35cc",
   "metadata": {},
   "source": [
    "## 4 - Correlation Analysis\n",
    "\n",
    "The time series are heavily correlated one with the other. We leverage this fact to reduce dimensionality with PCA. Now, we want to have a closer look at the correlation structure of the process. This will help us find the size of the temporal window we are going to use to construct PCA with less variables (and hopefully, better detection of faulty batches).\n",
    "\n",
    "Lagged Cross-Correlation Matrix:\n",
    "\n",
    "For each lag ll, compute a cross-correlation matrix C(l)C(l), which contains correlations between each pair of variables at time tt and t−lt−l. Summarize C(l)C(l) to a single measure, such as the average cross-correlation or maximum cross-correlation across all pairs. This will give you a measure of how much variables are correlated with their past values at each lag ll.\n",
    "\n",
    "The triangle comes from the fact that I have a BIASED correlation: points at the beginning and end of the series have less data to be evaluated. The triangular shape happens when there is a biased (0-padded, or finite) correlation between signals. To remove this triangular shape, one can simply remove the DC component/ the offset of the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot:\n",
    "# I am going to see correlations, but the more shifted they are, the less meaningful they are.\n",
    "# Maybe this does not help with finding the size of the temporal window?\n",
    "autocorrelated_all_operator_batches = list()\n",
    "for name, batch in grouped_batches:\n",
    "    # Compute the cross-correlation of the series with itself: (Could also use the Fourier transforms from above)\n",
    "    mean_1 = batch['Oxygen Uptake Rate(OUR:(g min^{-1}))'].mean()\n",
    "    mean_2 = batch['Oxygen in percent in off-gas(O2:O2  (%))'].mean()\n",
    "    autocorrelation = np.convolve(batch['Oxygen Uptake Rate(OUR:(g min^{-1}))'] - mean_1, \n",
    "                                  batch['Oxygen in percent in off-gas(O2:O2  (%))'].iloc[::-1] - mean_2)\n",
    "    # Normalize the result\n",
    "    autocorrelation = autocorrelation / (np.linalg.norm(batch['Oxygen Uptake Rate(OUR:(g min^{-1}))']) \\\n",
    "                                         * np.linalg.norm(batch['Oxygen in percent in off-gas(O2:O2  (%))']))\n",
    "    autocorr_operator_batch = pd.Series(autocorrelation)\n",
    "    autocorrelated_all_operator_batches.append(autocorr_operator_batch)\n",
    "\n",
    "autocorrelated_all_operator_batches = pd.concat(autocorrelated_all_operator_batches, axis=1)\n",
    "n = len(autocorrelated_all_operator_batches)\n",
    "autocorrelated_all_operator_batches['time shift'] = np.arange(-n // 2, n // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_list_3 = autocorrelated_all_operator_batches.columns\n",
    "variable_plot_selection_3 = widgets.Dropdown(options=variable_list_3)\n",
    "variable_plot_selection_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82caf693",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelated_all_operator_batches.plot(x='time shift', y=variable_plot_selection_3.value)\n",
    "ax.set_xlabel('cross-autocorrelation')\n",
    "ax.set_ylabel(variable_plot_selection_3.value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f70e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
